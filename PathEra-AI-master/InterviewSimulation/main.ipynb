{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reducing error', 'optimizing weights', 'neural network']\n",
      "0.6887084\n",
      "0.7833654\n",
      "0.7360369265079498\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from rake_nltk import Rake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "# model = SentenceTransformer('model/multiple_negatives')\n",
    "rake = Rake()\n",
    "\n",
    "data = pd.read_csv(\"data_science_questions.csv\")\n",
    "data = data.dropna(subset=[\"Topic\"])\n",
    "\n",
    "question = \"What is backpropagation?\"\n",
    "reference_answers = data[data[\"Question\"] == question][\"Answer\"].str.lower().tolist()\n",
    "topic = data[data[\"Question\"] == question][\"Topic\"].values[0]\n",
    "topic = topic.lower().split(\",\")\n",
    "# topic = data['Topic'].tolist()\n",
    "user_answer = \"Backpropagation is a way of determining the structure of a neural network by deciding the number of layers and nodes. It is unrelated to optimizing weights or reducing error after the model is trained.\"\n",
    "user_answer = user_answer.lower()\n",
    "\n",
    "def extract_keywords(text, min_length=2, max_length=6):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases()\n",
    "    keywords = [keyword for keyword in keywords if len(keyword.split()) >= min_length and len(keyword.split()) <= max_length]\n",
    "    return keywords\n",
    "\n",
    "def get_keyword_embeddings(keywords):\n",
    "    if not keywords: \n",
    "        return torch.zeros((1, model.get_sentence_embedding_dimension()))  \n",
    "    return model.encode(keywords, convert_to_tensor=True)\n",
    "\n",
    "def aggregate_embeddings(embeddings):\n",
    "    if embeddings.size(0) == 0:\n",
    "        return torch.zeros(model.get_sentence_embedding_dimension())\n",
    "    return embeddings.mean(dim=0)\n",
    "\n",
    "def preprocess_reference_answers(reference_answers):\n",
    "    for t in topic:\n",
    "        reference_answers = [answer.replace(t, \"\") for answer in reference_answers]\n",
    "    return reference_answers\n",
    "\n",
    "def preprocess_user_answer(user_answer):\n",
    "    for t in topic:\n",
    "        user_answer = user_answer.replace(t, \"\")\n",
    "    return user_answer\n",
    "\n",
    "def calculate_keyword_similarity(user_answer, reference_answers):\n",
    "    user_keywords = extract_keywords(user_answer)\n",
    "    reference_keywords_list = [extract_keywords(answer) for answer in reference_answers]\n",
    "\n",
    "    user_embeddings = get_keyword_embeddings(user_keywords)\n",
    "    reference_embeddings = [get_keyword_embeddings(keywords) for keywords in reference_keywords_list]\n",
    "\n",
    "    user_embedding = aggregate_embeddings(user_embeddings)\n",
    "    reference_embeddings = [aggregate_embeddings(ref_embeddings) for ref_embeddings in reference_embeddings]\n",
    "\n",
    "    similarities = []\n",
    "    for ref_embedding in reference_embeddings:\n",
    "        similarity = cosine_similarity(user_embedding.unsqueeze(0).cpu().numpy(), ref_embedding.unsqueeze(0).cpu().numpy())\n",
    "        similarities.append(similarity[0][0])\n",
    "    return similarities\n",
    "\n",
    "def calculate_similarity(user_answer, reference_answers):\n",
    "    user_embedding = model.encode(user_answer)\n",
    "    reference_embeddings = model.encode(reference_answers)\n",
    "\n",
    "    similarities = cosine_similarity([user_embedding], reference_embeddings)\n",
    "    return similarities[0]\n",
    "\n",
    "reference_answers = preprocess_reference_answers(reference_answers)\n",
    "user_answer = preprocess_user_answer(user_answer)\n",
    "print(extract_keywords(user_answer))\n",
    "similarity = max(calculate_similarity(user_answer, reference_answers))\n",
    "keyword_similarity = max(calculate_keyword_similarity(user_answer, reference_answers))\n",
    "print(similarity)\n",
    "print(keyword_similarity)\n",
    "print(similarity * 0.5 + keyword_similarity * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data_science_questions.csv\")\n",
    "question = \"What is the exploration-exploitation dilemma in reinforcement learning?\"\n",
    "data.loc[data['Question'] == question, 'Topic'] = \"Reinforcement Learning,exploration-exploitation dilemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_science_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
